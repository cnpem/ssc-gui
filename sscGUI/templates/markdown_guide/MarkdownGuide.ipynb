{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User Guide for ssc-cdi Jupyter Interfaces:\n",
    "\n",
    "To access JupyterLab, open the following address in the browser: [http://10.31.31.16/](http://10.31.31.16/).\n",
    "\n",
    "This interface runs at Bertha. There a few templates that can be accessed  in Template > gcc > /gcc/template_name.ipynb.\n",
    "\n",
    "## 1. Ptychography Interface:\n",
    "\n",
    "![Inteface!](images/ptycho01.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "Running the first Jupyter cell will prompt you to connect using your credentials and then install all the required packages for running the interface. \n",
    "\n",
    "The second cell deploys the graphical interface. Each tab of the interface is explained below \n",
    "\n",
    "#### ****0. Interface Header****\n",
    "\n",
    "![Inteface!](images/ptycho02.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "The goal of the interface is to aid on setting the parameters for the ptychography reconstruction. After having determined all the input parameters as wanted, these must be saved to a json file (**Save Inputs** button), and then run the ptychography (**Run Ptycho** button). \n",
    "\n",
    " - Machine selector: The jupyter interface runs locally at Bertha. This means all the interactions/plots that are shown on screen are being processed by Bertha. The machine selector option will determine on which machine you will run the ptychography algorithm in the end, after having determined and saved the input parameters;\n",
    "\n",
    "- Reset JSON: this loads a template JSON input file and overrides any modification you have made to the inputs in the first Tab;\n",
    "\n",
    " - GPUs slider: selects the number of GPUs to be used in the machine during processing. GPUs are used in the ptycho algorithm;\n",
    " \n",
    " - CPUS slider: selects the number of CPUs to be used in processing. For each GPU used, the rule of thumb is to use 32 CPUs.\n",
    " \n",
    " - Queue text field: text to select which cluster queue to use when running the algorithm using **Cluster**. This should in principle always be the **cat-proc** queue;\n",
    " \n",
    " - Job name text field: text to label the job you send to the queue; recommended to change if you want to run multiple jobs, so you can properly track each one.\n",
    "\n",
    "\n",
    "#### ****1. Ptychography Input Fields****\n",
    "\n",
    "![Inteface!](images/ptycho03.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "This corresponds to the input value of the JSON file read by the ptychography code. Set these according to your needs and with the aid of the subsquent Tabs of the interface. We provide below a bried explanation of each input:\n",
    "\n",
    "- **Proposal Path**: this is the path of the proposal containing the data folder. For example, for proposal number 00000000 it should be '/ibira/lnls/beamlines/caterete/apps/jupyter/00000000/data/ptycho2d/' or '/ibira/lnls/beamlines/caterete/apps/jupyter/00000000/data/ptycho3d/' for 2D and 3D datasets, respectively\n",
    "- **Data Folders**: a list containing the different data folders of interest inside the \"Proposal Path\" chosen in the first field. For instance, if the acquisition was divided into 3 folders named \"Sample_001\", \"Sample_002\" and \"Sample_004\", it should be written as ['Sample_001','Sample_002','Sample_004'] (between square brackets, and each item enclosed by quotes).\n",
    "- **projections**: a list that indicates which frame to proccess in each of the folders indicated in the \"Data Folders\" field. To process all frames, use an empty square bracket []. For instance, if you list [0, 10, 20], a total of 3 frames will be processed (for each folder!), these being the 1st, 11th and 21th frames, respectively.\n",
    "- **Center row/column**: this is the pixel value of the center of the diffraction pattern, determined from the \"Find Center\" tab.\n",
    "- **Diameter**: this the size of the square region of the diffraction pattern selected after restauration. This value needs to be small enough to fit a valid region of the diffraction pattern according to the center chosen above. For instance, since the detector has 3072x3072 pixels, if the center is at point (3072/2,3072/2) = (1536,1536) you may use a \"diameter\" of 3072 pixels. Nonetheless, if the center is displaced by 100 pixels, say (1436,1436), the maximum value of the \"diameter\" must be 2*(1536-100)=2*1436=2872 pixels.\n",
    "- **Save or load**: selects if you want to save the restaured diffraction patterns or skip this part and simply load them. The first time you run the algorithm you must use the \"save\" option. If you change the center of the diffraction pattern or the diameter above, you will need to use the \"Save\" option again. \n",
    "- **Autocrop borders**: this checkbox selects if you want to automatically crops the noisy borders of the reconstructed object, according to the positions of the scanning probe.\n",
    "- **Use Central mask**: this checkbox is determined by the \"Find Center\" Tab, indicating if you want to ignore a circular central portion of the diffraction pattern.\n",
    "- **Central mask Radius**: this is the radius of the circular mask selected above\n",
    "- **Ignore Detector Pileup**: this checkbox is determined by the \"Find Center\" Tab, indicating if you want to ignore the piled-up pixels in the central refion of the diffraction pattern.\n",
    "- **Probe Support Radius**: This is the radius of the support region left to reconstruct the probe, in pixels.\n",
    "- **Probe Center X/Y**: this is the center value, in pixels, of the circular probe support selected above. The value of (0,0) indicates the center of the image.\n",
    "- **Fresnel Number**: this is the fresnel number selected from the \"Probe Propagation\" Tab, used to evaluate the quality of the probe at the pinhole position.\n",
    "- **Probe incoherent_modes**: this indicated the number of probe modes to use in the reconstruction\n",
    "- **Recon Algorithm**: selection of the reconstruction algorithms and its respective parameters, as a dictionary.\n",
    "- **Phase Unwrap**: checkbox to phase unwrap the reconstructed object after the ptychography algorithm. Beware that for this to be successfull, the reconstruction should not contain the noisy borders, since these features mess up the unwrapping algorithm. \n",
    "- **FRC**: checkbox to select whether you want to calculate the Fourier-ring correlation of the reconstructed object. The phase signal is used for this calculation over the (phase-unwraped) signal.\n",
    "\n",
    "\n",
    "\n",
    "#### ****2. Ptycography****\n",
    "\n",
    "This Tab is used to start the ptychography algorithm. Before starting, you need to save the JSON file using the \"Save Inputs\" button. After that, use \"Run Ptycho\" to start the code. It will run on the machine you selected in the Header of the GUI.\n",
    "\n",
    "For the specific case you selected the **Cluster**, you may click \"List Jobs\" to print the current jobs on the cluster at the Log Console and check if your job is running. The can be cancelled by inserting the Job ID in the \"Job ID number\" field and then clicking the \"Job cancel\" button.\n",
    "\n",
    "![Inteface!](images/ptycho04.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "After running the ptychography a first time, you will generate a diffraction pattern, probe and reconstruction so you can then run the next Tabs to properly adjust the input parameters.\n",
    "\n",
    "#### ****3. mask visualization****\n",
    "\n",
    "\n",
    "Here you can click the \"Load\" button to visualize the average of the raw diffraction patterns together with the mask, to evaluate if the mask is properly ignoring the bad pixels of the diffraction pattern.\n",
    "\n",
    "![Inteface!](images/ptycho05.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "\n",
    "#### ****4. Find Diffraction Pattern Center****\n",
    "\n",
    "First load the diffraction pattern. Zoom-in to the plot and rover the mouse over it to find the central pixel. Use the sliders to set the row/column of the central pixe. You can use two approaches to ignore central pixels of the diffraction pattern. First, you may use a central circular mask, which radius is set by the third slider. The second option is to select the \"Ignore Detector Pileup\" checkbox. This will ignore any pixels that have reached a photon count greater than 350000 counts/second in a region of 256x256 pixels around the central value that you choose.\n",
    "\n",
    "![Inteface!](images/ptycho06.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "\n",
    "#### ****5. Probe progation and Fresnel Number****\n",
    "\n",
    "In this tab we perform the propagation of the reconstructed probe according to fresnel number values. The goal is to propagate the probe to the pinhole position to evaluate the quality of the pinhole/probe at that point. For that, we create a video of the propagation. The first slider determines the number of frames in your video. The subsequent slides determine the starting and ending value of the fresnel number. The step of the fresnel number for each frame is given by (end_value-start_value)/number_of_frames. After these parameters, click the button to propagate the probe. The resulting video will appear on the left. Look for the most uniform probe as possible using the slider on the top of the plot. The value of the f-number is shown on the plot title. The best value should be inserted on the \"Fresnel Number\" field on the left.\n",
    "\n",
    "![Inteface!](images/ptycho07.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "\n",
    "#### ****6. Reconstruction Visualization****\n",
    "\n",
    "After the parameters have all been determined, run the ptychography again using the Ptychography Tab. Then, you can load the reconstruction phase (left plot), amplitude (central plot) and probe (right plot) using the button in this Tab. Use the slider to select which frame to visualize.\n",
    "\n",
    "![Inteface!](images/ptycho09.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "\n",
    "#### ****7. Manual Crop data****\n",
    "\n",
    "This Tab allows you to load the reconstruction (first button), visualize a slice of interest using the slider and then crop the sinogram in each direction using the four bottom sliders. After selecting the cropping values of interest, use the second button to save the cropped sinogram.\n",
    "\n",
    "![Inteface!](images/ptycho08.png \"Jupyter interface for Ptychography\")\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 2. Tomography Interface:\n",
    "\n",
    "Running the first Jupyter cell will prompt you to connect using your credentials and then install all the required packages for running the interface.\n",
    "\n",
    "Then, the second cell will deploy the graphical interface.\n",
    "\n",
    "![Interface!](images/tomo0.png \"Jupyter interface for Tomography\")\n",
    "\n",
    "\n",
    "Currently, a 3D reconstruction requires the following processing steps, each corresponding to a different tab of the interface:\n",
    "\n",
    "#### ****1. Selecting data and sorting it by rotation angle****\n",
    "\n",
    "Here you will need to indicate the data folders of interest for the tomographic reconstruction. These are:\n",
    " - Ibira Datapath: the folder of the proposal containing the acquisition folder. \n",
    " - Ibira Datafolders: a list containing the folder of the acquisition inside \"Ibira Datapath\" (or the multiple acquisitions for a single sample, when that is the case)\n",
    " - Ptycho sinogram path: full path to the .npy sinogram containing all frames, i.e. the output of the ptychography algorithm. This folder should be according to the standard defined by Caterete: 00000000/proc/recons/myDataFolder/\n",
    " \n",
    "IMPORTANT: all outputs are saved in the same folder of the Ptychography sinogram above!\n",
    " \n",
    "After selecting the correct folders, click the \"Sort Frames\" button. After frame are sorted, the plot should appear on the right (LOADING MIGHT TAKE A WHILE!) and can be controlled with the slider bar / play button.\n",
    "\n",
    "![Interface!](images/tomo1.png \"Jupyter interface for Tomography\")\n",
    "\n",
    "\n",
    "#### ****2. Cropping undesired borders of the dataset****\n",
    "\n",
    "This is required to perform adequate phase unwrapping, since the noisy border interfere with the unwrapping algorithm\n",
    "\n",
    "Click the \"Load Frames\" button to load the sorted data from the previous tab. Then, use the sliders to crop as wanted in each direction. Make sure to visualize if the cropping is adequate for all frames. After chosing, click \"Save cropped frames\" to save the cropped sinogram.\n",
    "\n",
    "![Interface!](images/tomo2.png \"Jupyter interface for Tomography\")\n",
    "\n",
    "\n",
    "#### ****3. Phase Unwrapping****\n",
    "\n",
    "First, load the cropped sinogram with the \"Load cropped frames\" button. Then, find all the frames that are bad and list them in the \"Bad frames\" field as a list of numbers in square brackets. You can play the video of all frames as slow as wanted by changing the time each frime is visible in the \"Time/frame\" box. After listing all bad frames, click \"Remove Bad frames\".\n",
    "\n",
    "To perform phase unwrapping, you can set the number of Unwrap Iterations (standard = 0), which will iteratively try to remove background gradient in the images. The Non-negativity checkbox will make all values positive after the iterations; the Gradient checkbox will to a final gradient removal after the non-negativity step. Both of these should be false, a priori.\n",
    "\n",
    "After that, click \"Perform Unwrap\" to start unwrapping. You can view the progress in the Log Console (View > Show Log Console). After unwrapping is done, the unwrapped frames should be visible on the right. Click \"Save unwrapped frames\" to save the unwrapped sinogram for the next step.\n",
    "\n",
    "![Interface!](images/tomo3.png \"Jupyter interface for Tomography\")\n",
    "\n",
    "\n",
    "#### ****4. Convex Hull****\n",
    "    \n",
    "This step is OPTIONAL. It consists in an approach to select region around the object and null any existing null outside thie region. This should be used only when such noise is significant. First, load the unwrapped sinogram. The \"Convex Hull Preview\" will apply Convex Hull only to the frame that is currently selected, so you can quickly adjust the parameters (Invert, Threshold, Opening, Erosing, ConvexHull) and visualize its results. If no region seems to be found, the first step is to check/uncheck the \"Invert\" box. The remaining parameters need to be adjusted empirically to find the best convex region around the sample (which will be shown in the plot entitled \"Convex Hull\"). The plot with name masked Image is the product of the Convex Hull mask with the original image, and it will be the frame saved to the output sinogram.\n",
    "\n",
    "After you have found a satisfactory combination of parameters, click \"Do complete Convex Hull\" to do it for all frames. After this is complete, click \"Load cHull sinogram\" to visualize all the frames with Convex Hull and check if any were bad. If so, these can be removed by inserting their number in the Bad Frames fields as a list in squared brackets, and clicking \"Remove Bad Frames\"\n",
    "    \n",
    "![Interface!](images/tomo4.png \"Jupyter interface for Tomography\")\n",
    "    \n",
    "    \n",
    "#### ****5. Wiggle****\n",
    "\n",
    "This step consists in the alignment of the shifted frames of the sinogram, with respect to a reference frame. This is the most time consuming part of the processing.\n",
    "\n",
    "You can either load the sinogram from Phase Unwrapping or Convex Hull. \n",
    "\n",
    "After loading the tomogram, select any bad frames from the previous steps to be removed in list format again. \n",
    "\n",
    "Then, you will need to project the angles to a regular mesh of angle values using the \"Project Angles\" button. The size of the angle step is determined from the \"Angle step\" slider, which indicates a percentage. If Angle step = 100, it will use the biggest angle step between angles in the original values; else if Angle step = 0, it will use the minimum; intermediary values between 0 and 100 results in values between min and max. We recommend starting at 100. If this results in too many lost angles, gradually decrease the slider. Click the button to project the angles. After that, the projected frames should be used to find a Reference Frame with respect to which alignment will be performed. We recommend this frame to have the sample as centered as possible. Choose the number of CPUs to use in this part, which is performed in parallel CPUs. We recommends at least 32. \n",
    "\n",
    "Click \"Perform Wiggle\". After it is done, click \"Load Wiggle\" to show wiggled frames on the right-most plot.\n",
    "\n",
    "You can visualize the different slices of the sinogram in the XY and XZ directions with the Sinogram sliders. \n",
    "\n",
    "IMPORTANT: the \"Invert Sinogram\" button is used to overwrite the wiggled sinogram with the opposite sine (i.e. times -1). This should be done when the sinogram presents negative values for the sample (dark) and positive (bright) for the background.\n",
    "\n",
    "![Interface!](images/tomo6.png \"Jupyter interface for Tomography\")\n",
    "\n",
    "\n",
    "#### ****6. Tomography**** \n",
    " \n",
    " - Currently, we can only use the tomography EEM algorithm. We reccomend trying it without regulirization at first.\n",
    " - The regularization parameter can be played with to try and improve the reconstruction quality. \n",
    " - The number of iterations will not necessarily improve reconstruction quality as it gets bigger. In fact, too many iterations might deteriorate quality for some samples. We recommend values between 20 and 40 to start with. \n",
    " - GPUs list: should be a list containing the indexes for each GPU to be used. For instance [0,1,2,3] if you want to use 4 GPUs; [0,1] if you want use 2 GPUs. [0,1,2] if you want 3 GPUs.\n",
    " - N of CPUS: The recommendation is to select 32 CPUs for each GPU used.\n",
    " - Machine Queue: this is the cluster queue to be used when you are running this with via Mafalda. Ignore it if you are using Bertha.\n",
    " - Slurm Job Name: the name of the job that will be sent to the cluster when you run it via Mafalda.\n",
    " \n",
    " After adjusting these parametes, select which steps of the tomographic processing you want to perform. If you already created the sinograms in the previous steps, you may select only the \"Tomo\" checkbox. Otherwise, you can select the one you want to re-run any steps, as wanted. Click \"Save JSON\" to save the file with the input parameters for the tomography. Then click \"Start\". \n",
    " \n",
    " Check the LogConsole to see when the tomography finishes. Then, click \"Load Recon Slices\" to visualize the frames in each direction. These can be controlled using the sliders. In some cases, the reconstruction present outlier values that do not allow one to properly visualize anything in the plots. In that case you can set a threshold value and save a thresholded version of the 3D reconstruction. For instance, if you set it to 100, any voxel presenting value above 100 or below -100 will be set to zero. \n",
    " \n",
    "All reconsutructions are saved both in numpy and tif format!\n",
    "\n",
    "After all steps are done, you can click the \"Delete temporaty files\" button to erase all intermediary sinograms that were generated, that is, you will be left only with the original Ptychography sinogram and the 3D reconstruction files.\n",
    "\n",
    "![Interface!](images/tomo8.png \"Jupyter interface for Tomography\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
